---
title: "Classification Trees"
format: beamer
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see <https://quarto.org/docs/presentations/>.

## Bullets

When you click the **Render** button a document will be generated that includes:

-   Content authored with markdown
-   Output from executable code

## Code

When you click the **Render** button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```

## Reading Materials

-   Max Kuhn. Chapter 14. Section 14.1

## Decision Boundary in Classification

![](images/db.png)

Classification is a process of finding the **decision boundary** that best separate two classes

## Decision Boundary in Classification

![](images/db1.png)

SVM = Support Vector Machine

## Decision Tree

-   Decision Tree for classification is **Classification Tree**
-   Decision Tree for Regression is **Regression Tree**

## Example of Classification Tree

![](images/tree2.jpg)

\[Link\] (http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)

## Classification Tree

-   In two dimension, classification Tree's decision boundary is a collection of horizontal and vertical line

## Data

```{r, echo = FALSE}
x1 = c(1:4,4)
x2 = c(1,2,1,0,1)
y = c(0,1,0,1,1)

d = data.frame(cbind(x1, x2, y))
library(ggplot2)
ggplot(d, aes(x=x1, y=x2, color = factor(y)))+geom_point(size =7)+ylim(0, 10)+
  theme_bw() +
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()) +
  theme(legend.position = "none")

```

-   The tree starts by a vertical or horizontal line that **best** seperate the data
-   **Question**: Find a vertical line that best seperate **red** and **green**.

## One way to seperate the reds and greens

```{r, echo = FALSE}
x1 = c(1:4,4)
x2 = c(1,2,1,0,1)
y = c(0,1,0,1,1)

d = data.frame(cbind(x1, x2, y))
library(ggplot2)
ggplot(d, aes(x=x1, y=x2, color = factor(y)))+geom_point(size = 7)+ylim(0, 10)+geom_vline(xintercept=3/2, linetype="dashed", color = "blue", size=1)+ geom_text(x=3/2, y=.8, label="Split 1", size = 10)+
  theme_bw() +
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()) +
  theme(legend.position = "none")

```

## One way to seperate the reds and greens

```{r, echo = FALSE}
x1 = c(1:4,4)
x2 = c(1,2,1,0,1)
y = c(0,1,0,1,1)

d = data.frame(cbind(x1, x2, y))
library(ggplot2)
ggplot(d, aes(x=x1, y=x2, color = factor(y)))+geom_point(size = 7)+ylim(0, 10)+geom_vline(xintercept=5/2, linetype="dashed", color = "blue", size=1)+ geom_text(x=5/2, y=.8, label="Split 2", size=10)+
  theme_bw() +
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()) +
  theme(legend.position = "none")

```

## One way to seperate the reds and greens

```{r, echo = FALSE}
x1 = c(1:4,4)
x2 = c(1,2,1,0,1)
y = c(0,1,0,1,1)

d = data.frame(cbind(x1, x2, y))
library(ggplot2)
ggplot(d, aes(x=x1, y=x2, color = factor(y)))+geom_point(size = 7)+ylim(0, 10)+geom_vline(xintercept=7/2, linetype="dashed", color = "blue", size=1)+ geom_text(x=7/2, y=.8, label="Split 3", size=10)+
  theme_bw() +
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()) +
  theme(legend.position = "none")

```

## Question

-   **Question**: Which is the best split?

## Partial Answer

-   It looks like Split 1 and 3 are better than Split 2 since it misclassifies less
-   Which is the better split between Split 1 and Split 3?
-   We need to find a way to measure *how good a split is*

## Impurity Measure

-   The impurity of a node (**a node = a subset of the data or the original data**) measure how uncertain the node is.\
-   For example, node A with 50% reds and 50% greens would be more uncertained than node B with 90% reds and 10% greens. Thus, node A has greater impurity than node B.
-   More uncertained $=$ Greater impurity

## Impurity Measure

-   A split that *gains* more impurity is the **better split**!

## Impurity Gain

![](images/parent_child.png){height=1.5in}

$$
IG = I_{parent} - \frac{N_{left}}{N}I_{left}-\frac{N_{right}}{N}I_{right}
$$

-   IG is Impurity Gain of the split
-   $N_{left}$ and $N_{right}$ are the number of points in the left child node and right child node, respectively.
-   $N_{left}+N_{right}=N$

## Impurity Measure

-   Impurity can be measured by: classification error, Gini Index, and Entropy.

## Impurity Measure

-   Let $p_0$ and $p_1$ be the proportion of class 0 and class 1 in a node.

\begin{align*}
{\text{By Classification Error: }} I &= min\{p_0, p_1\} \\
{\text{By Gini Index: }} I&= 1 - p_0^2-p_1^2 \\
{\text{By Entropy: }} I &= -p_0 \log_2(p_0)-p_1\log_2(p_1) 
\end{align*}

## Calculation

-   Let's calculate the impurity gain of the three splits to decide which split is the best

## IG By Classification Error

![](images/im1.png){height=1in}

-   Let **green** and **red** be class 0 and class 1, respectively.

For Split 1: $N = 5, N_{left} =1, N_{right} = 4$

-   Node *parent,* A: $p_0 = \frac{2}{5}, p_1 = \frac{3}{5}$. Thus, $I_{A} = \text{min}(\frac{2}{5}, \frac{3}{5}) = \frac{2}{5}$

-   Node *child left,* L: $p_0 = \frac{0}{1} = 0, p_1 = \frac{1}{1} = 1$. Thus, $I_{L} = \text{min}(0, 1) = 0$

-   Node *child right,* R: $p_0 = \frac{3}{4}, p_1 = \frac{1}{4}$. Thus, $I_{R} = \text{min}(\frac{3}{4}, \frac{1}{4}) = \frac{1}{4}$

-   Impurity Gain of Split 1:

$$IG = \frac{2}{5} - \frac{1}{5} \cdot 0-\frac{4}{5} \cdot \frac{1}{4} = 0.2$$

## IG By Classification Error

![](images/im3.png){height=1in}

For Split 2: $N = 5, N_{left} =2, N_{right} = 3$

-   Node *parent,* A: $p_0 = \frac{2}{5}, p_1 = \frac{3}{5}$. Thus, $I_{A} = \text{min}(\frac{2}{5}, \frac{3}{5}) = \frac{2}{5}$

-   Node *child left,* L: $p_0 = \frac{1}{2}, p_1 = \frac{1}{2}$. Thus, $I_{L} = \frac{1}{2}$

-   Node *child right,* R: $p_0 = \frac{2}{3}, p_1 = \frac{1}{3}$. Thus, $I_{R} = \text{min}(\frac{2}{3}, \frac{1}{3}) = \frac{1}{3}$

-   Impurity Gain of Split 2:

$$IG = \frac{2}{5} - \frac{2}{5} \cdot \frac{1}{2}-\frac{3}{5} \cdot \frac{1}{3} = 0$$

## IG By Classification Error

For Split 3: $N = 5, N_{left} =3, N_{right} = 2$

![](images/im2.png){height=1in}

-   Node *parent,* A: $p_0 = \frac{2}{5}, p_1 = \frac{3}{5}$. Thus, $I_{A} = \text{min}(\frac{2}{5}, \frac{3}{5}) = \frac{2}{5}$

-   Node *child left,* L: $p_0 = \frac{1}{3}, p_1 = \frac{2}{3}$. Thus, $I_{A} = \text{min}(\frac{1}{3}, \frac{2}{3}) = \frac{1}{3}$

-   Node *child right,* R: $p_0 = \frac{2}{2}, p_1 = \frac{0}{2}$. Thus, $I_{R} = \text{min}(1,0) = 0$

-   Impurity Gain of Split 3:

$$IG = \frac{2}{5} - \frac{3}{5} \cdot \frac{1}{3}-\frac{2}{5} \cdot 0 = 0.2$$

## Comparing IG By Classification Error

| Split        | IG  |
|---------|-----|
|  1 | 0.2 |
|  2 | 0   |
|  3 | 0.2 |

-   By classification error, Split 1 and Split 3 are tie as the best because they have the same impurity gain.

## lol

- lol


